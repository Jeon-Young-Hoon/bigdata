{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "you must first build vocabulary before training the model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-44f256554286>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;31m# model.init_sims(replace=True)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;31m# vocab,vocab_vector = model.wv.vocab(), model[vocab]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    854\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    855\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 856\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_sentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1e6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks)\u001b[0m\n\u001b[0;32m    936\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 938\u001b[1;33m             queue_factor=queue_factor, report_delay=report_delay, compute_loss=compute_loss, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_job_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    419\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 421\u001b[1;33m             total_words=total_words, **kwargs)\n\u001b[0m\u001b[0;32m    422\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_check_training_sanity\u001b[1;34m(self, epochs, total_examples, total_words, **kwargs)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# should be set by `build_vocab`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must first build vocabulary before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"you must initialize vectors before training the model\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: you must first build vocabulary before training the model"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "\n",
    "#word2벡터 부분\n",
    "model= Word2Vec()\n",
    "\n",
    "vocab,vocab_vector = model.wv.vocab(), model[vocab]\n",
    "\n",
    "\n",
    "n_cluster = 20\n",
    "inertia_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<konlpy.corpus.CorpusLoader at 0x1a9a2438710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3,n_cluster):\n",
    "\n",
    "    km = KMeans(n_clusters=i)\n",
    "\n",
    "    km.fit(vecs)\n",
    "    \n",
    "    inertia_list.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(inertia_list)\n",
    "plt.show() # 여기서 5처럼 갑자기 경사가 낮아지는곳 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa0 1\n",
      "setosa1 1\n",
      "setosa2 1\n",
      "setosa3 1\n",
      "setosa4 1\n",
      "setosa5 1\n",
      "setosa6 1\n",
      "setosa7 1\n",
      "setosa8 1\n",
      "setosa9 1\n",
      "setosa10 1\n",
      "setosa11 1\n",
      "setosa12 1\n",
      "setosa13 1\n",
      "setosa14 1\n",
      "setosa15 1\n",
      "setosa16 1\n",
      "setosa17 1\n",
      "setosa18 1\n",
      "setosa19 1\n",
      "setosa20 1\n",
      "setosa21 1\n",
      "setosa22 1\n",
      "setosa23 1\n",
      "setosa24 1\n",
      "setosa25 1\n",
      "setosa26 1\n",
      "setosa27 1\n",
      "setosa28 1\n",
      "setosa29 1\n",
      "setosa30 1\n",
      "setosa31 1\n",
      "setosa32 1\n",
      "setosa33 1\n",
      "setosa34 1\n",
      "setosa35 1\n",
      "setosa36 1\n",
      "setosa37 1\n",
      "setosa38 1\n",
      "setosa39 1\n",
      "setosa40 1\n",
      "setosa41 1\n",
      "setosa42 1\n",
      "setosa43 1\n",
      "setosa44 1\n",
      "setosa45 1\n",
      "setosa46 1\n",
      "setosa47 1\n",
      "setosa48 1\n",
      "setosa49 1\n",
      "versicolor50 0\n",
      "versicolor51 0\n",
      "versicolor52 0\n",
      "versicolor53 3\n",
      "versicolor54 0\n",
      "versicolor55 0\n",
      "versicolor56 0\n",
      "versicolor57 3\n",
      "versicolor58 0\n",
      "versicolor59 3\n",
      "versicolor60 3\n",
      "versicolor61 0\n",
      "versicolor62 3\n",
      "versicolor63 0\n",
      "versicolor64 3\n",
      "versicolor65 0\n",
      "versicolor66 0\n",
      "versicolor67 3\n",
      "versicolor68 0\n",
      "versicolor69 3\n",
      "versicolor70 0\n",
      "versicolor71 3\n",
      "versicolor72 0\n",
      "versicolor73 0\n",
      "versicolor74 0\n",
      "versicolor75 0\n",
      "versicolor76 0\n",
      "versicolor77 0\n",
      "versicolor78 0\n",
      "versicolor79 3\n",
      "versicolor80 3\n",
      "versicolor81 3\n",
      "versicolor82 3\n",
      "versicolor83 0\n",
      "versicolor84 3\n",
      "versicolor85 0\n",
      "versicolor86 0\n",
      "versicolor87 0\n",
      "versicolor88 3\n",
      "versicolor89 3\n",
      "versicolor90 3\n",
      "versicolor91 0\n",
      "versicolor92 3\n",
      "versicolor93 3\n",
      "versicolor94 3\n",
      "versicolor95 3\n",
      "versicolor96 3\n",
      "versicolor97 0\n",
      "versicolor98 3\n",
      "versicolor99 3\n",
      "virginica100 2\n",
      "virginica101 0\n",
      "virginica102 4\n",
      "virginica103 2\n",
      "virginica104 2\n",
      "virginica105 4\n",
      "virginica106 3\n",
      "virginica107 4\n",
      "virginica108 2\n",
      "virginica109 4\n",
      "virginica110 2\n",
      "virginica111 2\n",
      "virginica112 2\n",
      "virginica113 0\n",
      "virginica114 2\n",
      "virginica115 2\n",
      "virginica116 2\n",
      "virginica117 4\n",
      "virginica118 4\n",
      "virginica119 0\n",
      "virginica120 2\n",
      "virginica121 0\n",
      "virginica122 4\n",
      "virginica123 0\n",
      "virginica124 2\n",
      "virginica125 4\n",
      "virginica126 0\n",
      "virginica127 0\n",
      "virginica128 2\n",
      "virginica129 4\n",
      "virginica130 4\n",
      "virginica131 4\n",
      "virginica132 2\n",
      "virginica133 0\n",
      "virginica134 0\n",
      "virginica135 4\n",
      "virginica136 2\n",
      "virginica137 2\n",
      "virginica138 0\n",
      "virginica139 2\n",
      "virginica140 2\n",
      "virginica141 2\n",
      "virginica142 0\n",
      "virginica143 2\n",
      "virginica144 2\n",
      "virginica145 2\n",
      "virginica146 0\n",
      "virginica147 2\n",
      "virginica148 2\n",
      "virginica149 0\n"
     ]
    }
   ],
   "source": [
    "#만약에 5라면 아래를 통해 어디 클러스터에 속하는지 알수 있다.\n",
    "final_cluster = 5\n",
    "km = KMeans(n_clusters=final_cluster)\n",
    "pred = km.fit_predict(vecs)\n",
    "for i,j in zip(word,pred):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_sentences = [['setosa4','virginica141'],\n",
    "                ['virginica126','versicolor86']] # 절별로 짜른거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 0, 0],\n",
       "       [2, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_sentences = []\n",
    "for tmp_sentence in tmp_sentences:\n",
    "    tmp_list=[]\n",
    "    for i in tmp_sentence:\n",
    "        tmp_list.append(pred[word.index(i)])\n",
    "#     print(tmp_list)\n",
    "    res_sentences.append(np.bincount(tmp_list,minlength=final_cluster))\n",
    "np.array(res_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lex_dict():\n",
    "    positive_dict=['virginica141','좋','괜찮','진리','천재','꿀','짱','강추','장점','최고']\n",
    "    negative_dict=['versicolor86','힘들','벅차','무섭','귀찮','바쁘','아갑','늦','아쉽','두렵','별로']\n",
    "    return positive_dict,negative_dict\n",
    "\n",
    "positive_dict,negative_dict = load_lex_dict()\n",
    "def what_pos(sentence):\n",
    "    pos_case=[]\n",
    "    \n",
    "    for pd in positive_dict:\n",
    "        if pd in sentence:\n",
    "            print(pd)\n",
    "            pos_case.append(pd)\n",
    "    return pos_case\n",
    "\n",
    "def what_neg(sentence):\n",
    "    neg_case=[]\n",
    "    for nd in negative_dict:\n",
    "        if nd in sentence:\n",
    "            print(nd)\n",
    "            neg_case.append(nd)\n",
    "    return neg_case\n",
    "\n",
    "def senetene_labelling(ex_sentences):\n",
    "    \n",
    "    res_list=[]\n",
    "    for i,sentence in enumerate(ex_sentences):\n",
    "        \n",
    "        tmp_list=[]\n",
    "        \n",
    "        pos_list = what_pos(sentence)\n",
    "        neg_list = what_neg(sentence)\n",
    "        \n",
    "        res_list.append(float(len(pos_list) >= len(neg_list)))\n",
    "    \n",
    "        print(sentence,len(pos_list),len(neg_list))\n",
    "\n",
    "\n",
    "#         res_list.append([sentence,len(pos_list),len(neg_list)])\n",
    "            \n",
    "#         # debug\n",
    "#         print(i,'번째 문장: ',sentence)\n",
    "\n",
    "#         print('--positive case--\\n',pos_list)\n",
    "\n",
    "#         print('\\n\\n')\n",
    "\n",
    "#         print('--negative case--\\n',neg_list)\n",
    "\n",
    "#         print('\\n\\n')\n",
    "    return res_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "virginica141\n",
      "['setosa4', 'virginica141'] 1 0\n",
      "versicolor86\n",
      "['virginica126', 'versicolor86'] 0 1\n"
     ]
    }
   ],
   "source": [
    "X = np.array(res_sentences)\n",
    "y = senetene_labelling(tmp_sentences)\n",
    "    \n",
    "# y = np.ones(len(X)) # 우리가 아직 라벨링을 안했으니까 임시로\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "batchSize=1\n",
    "learningRate = 0.01\n",
    "epochNum = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 0 0]\n",
      " [2 0 0 0 0]] [1.0, 0.0]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \n",
    "    def __init__(self,train_or_test=\"train\"):\n",
    "        if train_or_test == \"train\":\n",
    "            self.X=X_train\n",
    "            self.y=y_train\n",
    "        elif train_or_test == \"test\":\n",
    "            self.X=X_test\n",
    "            self.y=y_test\n",
    "        else:\n",
    "            self.X=X\n",
    "            self.y=y\n",
    "    def __getitem__(self,index):\n",
    "        return torch.from_numpy(X[index]).type(torch.FloatTensor),y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Mydataset('train')\n",
    "train_loader = DataLoader(train_dataset,batch_size=batchSize,shuffle=True)\n",
    "\n",
    "test_dataset = Mydataset('test')\n",
    "test_loader = DataLoader(test_dataset,batch_size=batchSize,shuffle=True)\n",
    "\n",
    "tmp_dataset=Mydataset('tmp')\n",
    "tmp_loader=DataLoader(tmp_dataset,batch_size=2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mynet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Mynet, self).__init__()\n",
    "        \n",
    "        #self.hidden1 = nn.Linear(final_cluster,10)\n",
    "        self.output = nn.Linear(5,1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         z = self.hidden1(x)\n",
    "#         z = nn.Sigmoid(z)\n",
    "        z = self.output(x)\n",
    "        \n",
    "        return z\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Mynet()\n",
    "model.train()                                                          \n",
    "costFunction = nn.BCELoss()                                   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learningRate)    \n",
    "\n",
    "totalStep = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "before outputs\n",
      "epoch:  1\n",
      "before outputs\n",
      "epoch:  2\n",
      "before outputs\n",
      "epoch:  3\n",
      "before outputs\n",
      "epoch:  4\n",
      "before outputs\n",
      "epoch:  5\n",
      "before outputs\n",
      "epoch:  6\n",
      "before outputs\n",
      "epoch:  7\n",
      "before outputs\n",
      "epoch:  8\n",
      "before outputs\n",
      "epoch:  9\n",
      "before outputs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jyh54\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1594: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "for epoch in range(epochNum):\n",
    "    print('epoch: ',epoch)\n",
    "    for idx, (sen_bins, labels) in enumerate(tmp_loader):\n",
    "        \n",
    "        tmp_X = Variable(sen_bins)\n",
    "        tmp_y = labels\n",
    "        \n",
    "        #labels = labels.view(-1,1)\n",
    "        # Forward pass\n",
    "        print('before outputs')\n",
    "        outputs = model(tmp_X)    \n",
    "        loss = costFunction(outputs, tmp_y.float()) \n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (idx + 1) % 100 == 0:\n",
    "            \n",
    "            print(\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\".format(epoch + 1, epochNum, idx + 1, totalStep, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
